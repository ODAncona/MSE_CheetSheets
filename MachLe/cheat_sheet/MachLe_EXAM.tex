%
% Packages
%
\documentclass{article}
\usepackage[landscape]{geometry}
\usepackage{url}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{bigints}
\usepackage{amsfonts}
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{amsmath,amssymb}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{amsmath,amssymb}
\usepackage{enumitem}
\usepackage{xhfill}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{parskip}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}
\makeatletter

%
% Math
%
\newcommand{\Real}{\mathbb R}
\newcommand{\RPlus}{\Real^{+}}
\newcommand{\norm}[1]{\left\Vert#1\right\Vert}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\setn}[1]{\left\{#1\right\}_{\scriptscriptstyle n \ge 1}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\seq}[1]{\left<#1\right>}
\newcommand{\eps}{\varepsilon}
\newcommand{\To}{\longrightarrow}
\newcommand{\Prob}{\rm{P}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\h}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\E}{{\rm E}}
\newcommand{\Hnull}{{\rm H}_{0}}
\newcommand{\Hone}{{\rm H}_{1}}
\newcommand{\Var}{{\rm Var}}
\newcommand{\Cov}{{\rm Cov}}
\newcommand{\sign}{{\rm sign}}
\newcommand{\med}{{\rm med}}
\newcommand{\tr}{{\rm tr}}
\newcommand{\T}{{\text{\tiny \rm T}}}
\newcommand{\minf}{- \, \infty}
\newcommand{\intervalle}[4]{\mathopen{#1}#2\mathpunct{},#3\mathclose{#4}}
\newcommand{\intervalleff}[2]{\intervalle{[}{#1}{#2}{]}}
\newcommand{\intervalleof}[2]{\intervalle{]}{#1}{#2}{]}}
\newcommand{\intervallefo}[2]{\intervalle{[}{#1}{#2}{[}}
\newcommand{\intervalleoo}[2]{\intervalle{]}{#1}{#2}{[}}
\newcommand*\conj[1]{\overline{#1}}
\newcommand*\mean[1]{\overline{#1}}

%
% Setup
%
\title{MachLe - Olivier D'Ancona}
\advance\topmargin-.8in
\advance\textheight3in
\advance\textwidth3in
\advance\oddsidemargin-1.5in
\advance\evensidemargin-1.5in
\parindent0pt
\parskip2pt
\linespread{0.4}


%
% Commands
%
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
\newcommand{\hr}{\centerline{\rule{3.5in}{1pt}}}
%\colorbox[HTML]{e4e4e4}{\makebox[\textwidth-2\fboxsep][l]{texto}
\newcommand{\nc}[2][]{%
\vspace{-.16cm}
\tikz \draw [draw=black, ultra thick, #1]
    ($(current page.center)-(0.5\linewidth,0)$) --
    ($(current page.center)+(0.5\linewidth,0)$)
    node [midway, fill=white] {#2};
}% tomado de https://tex.stackexchange.com/questions/179425/a-new-command-of-the-form-tex


%
% Styles
%
\tikzstyle{mybox} = [draw=black, fill=white, very thick,
rectangle, rounded corners, inner sep=2pt, inner ysep=7pt]
\tikzstyle{fancytitle} =[fill=black, text=white, font=\bfseries]

\newlength{\boxsize}
\setlength{\boxsize}{0.197\textwidth}
\raggedcolumns
%###############################################################################################
%
%                                         Document
%
%###############################################################################################

\begin{document}

%---------------------------------
% Title
%---------------------------------
\begin{center}
    {\huge{\textbf{MachLe - Olivier D'Ancona}}}\\
\end{center}
\vspace{-0.1cm}

\begin{multicols*}{5}

    %---------------------------------
    % Evaluation Metrics
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
            $Accuracy = \dfrac{TP + TN}{ TP + TN + FP + FN}$\\
            $Precision = \dfrac{TP}{ TP + FP}$\\
            $Recall = \dfrac{TP}{ TP + FN}$ \\
            $Specificity = \dfrac{TP}{TN + FP}$\\
            $Fscore = \dfrac{2\cdot Precision \cdot Recall}{ Precision + Recall}$\\
            $error$ $rate$ $= 1 - accuracy$\\
            $macro$ $average = \dfrac{1}{n} \sum_{i=1}^{n} avg_i$\\
           \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Evaluation Metrics};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Activation Functions
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                \textbf{Sigmoid}: $\sigma(x)=\dfrac{1}{1 + e^{-x}}$\\
                \textbf{Hyperbolic tangent}: $\frac {e^{x} - e^{-x}}{e^{x} + e^{-x}}$\\
                \textbf{Relu}: $\begin{cases}0 & \text{si $x < 0$ } \\x & \text{si $x \ge 0$} \\\end{cases}$\\
                \textbf{Gaussian}: $e^{-x^{2}}$\\
                \textbf{Softmax}: $\dfrac{e^{z_j}}{ \sum_{k=1}^{K}{e^{z_k}}}$\\
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Activation Functions};
    \end{tikzpicture}
    %---------------------------------


    %---------------------------------
    % Neural Network
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                \nc{Structure}\\
                \textbf{Biais}: $b$, An extra weight that can be learned using a learning algorithm. The purpose is to replace threshold.\\
                \textbf{Input}: $I$, Input vector
                \textbf{Weights}: $W$, Vector of weights
                \nc{Learning algorithm}\\
                \begin{enumerate}
                \item Randomly initialize weights
                \item Compute the neuron's output for a fiven input vector X
                \item Update weights: $W_j(t+1) = W_j(t) + \eta\left(\hat{y_i}-y\right)x$ with $\eta$ the learning rate and $\hat{y_i}$ the desired output.
                \item Repeat steps 2 and 3 for the number of epochs you need or until the error is smaller than a threshold.
                \end{enumerate}
           \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Neural Network};
    \end{tikzpicture}
    %---------------------------------



    %---------------------------------
    % KNN
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                Hyperparameters:\\
                \begin{itemize}[noitemsep]
                    \item Number of neighbours $k$
                    \item Distance metric
                    \item normalization type
                    \item strategy if no majority
                \end{itemize}
                Big $k$:\\
                (+) More confidence, probabilistic
                (-) No locality, heavier
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {KNN};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Bayes
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                $$ P(C_k|x) = \frac{P(x|C_k) \cdot P(C_k)}{P(x)} $$                où
                \begin{itemize}[noitemsep]
                    \item $C_k$: Classe ciblée
                    \item $x$: Évidence
                    \item $P(C_k)$: Probabilité a priori de la classe $C_k$
                    \item $P(x|C_k)$: probability of observing x given class j
                    \item $P(C_k|x)$: Probabilité a posteriori de la classe $C_k$ après observation de $x$
                    \item $P(x)$: Probabilité de l'évidence $x$
                \end{itemize}
                avec 
                $$ P(x) = \sum_{\text{toutes classes } C_k} P(x|C_k) \cdot P(C_k) $$

                \nc{Classifier H/F:}
                \begin{itemize}
                    \item $P(C_f) = \frac{4}{70}, P(C_g) = \frac{66}{70}$
                    \item $p(x|C_g) = 0.8, p(x|C_f) = 0.2$
                    \item Calcul de $p(x)$: 
                    $$ p(x) = 0.2 \times \frac{4}{70} + 0.8 \times \frac{66}{70} $$
                    \item Calcul de $P(C_f|x)$ et $P(C_g|x)$:
                    $$ P(C_f|x) = \frac{0.2 \times \frac{4}{70}}{p(x)}, \quad P(C_g|x) = \frac{0.8 \times \frac{66}{70}}{p(x)} $$
                \end{itemize}

                (+)Can deal with imbalanced dataset, prior can be changed
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Bayes};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Linear Regression
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{\boxsize}
                Soit un tableau de données: 

                $x$ = Surface(g) , $y$ = Price(cm) , $x \cdot y$ , $x^2$	  

                $$ X = [1, Surface] $$
                $$ X^TX = \begin{bmatrix} n & \sum{x_i} \\ \sum{x_i} & \sum{x_i^2} \end{bmatrix} = \begin{bmatrix} 7 & 38.5 \\ 38.5 & 218.95 \end{bmatrix} $$
                $$ X^Ty = \begin{bmatrix} \sum{y_i} \\ \sum{x_iy_i} \end{bmatrix} = \begin{bmatrix} 348 \\ 1975 \end{bmatrix} $$
                $$ \hat{\theta} = (X^TX)^{-1}X^Ty = \begin{bmatrix} \theta_0 \\ \theta_1 \end{bmatrix} = \begin{bmatrix} -2.67 \\ 9.51 \end{bmatrix} $$
                $$ \hat{y} = \theta_0 + \theta_1x $$
                \nc{Matrix Inversion (2x2)}
                $$ \begin{bmatrix} a & b \\ c & d \end{bmatrix}^{-1} = \dfrac{1}{ad-bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} $$
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Linear Regression};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Logistic Regression
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                $$ h_\theta(x_n) = \sigma(x \theta^T) $$
                \begin{itemize}[noitemsep]
                    \item $h_\theta(x_n)$ : predicted value
                    \item $\theta$ : model's parameters
                    \item $X$ : input vector
                \end{itemize}

                \textbf{Goal}: Find the $\theta$ that maximizes the likelihood of the data.\\
                \textbf{Loss}:
                $$
                \begin{aligned}
                J(\beta) &= -\frac{1}{n} \sum_{i=1}^{n} y_i \log(h_\theta(x_n)) +\\
                &\qquad \qquad(1 - y_i) \log(1 - h_\theta(x_n))
                \end{aligned}
                $$
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Logistic Regression};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Normalization
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
         \begin{minipage}{0.197\textwidth}
            \textbf{Min-max [0,1]}: $x' = \dfrac{\left(x-x_{min}\right)}{\left(x_{max}-x_{min}\right)}$\\
            \textbf{Min-max [-1,1]}: $x' = 2 \cdot min\_max(x) -1$ \\
            min-max doesn't handle outliers.\\
            \textbf{Z-norm}: $x' = \dfrac{\left(x-\mu\right)}{\sigma}$\\
         \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Normalization};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Support Vector Machine (SVM)
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                \textbf{Concept}: 
                SVM finds the hyperplane that best separates different classes by maximizing the margin between the closest points of different classes (support vectors).\\

                hw(x) = sign(b + wx)

                \nc{Formulation}
                $$ \max_{\omega, b} \frac{1}{\|\omega\|} \quad \text{s.t.} \quad y_i(\omega \cdot x_i + b) \geq 1 \, \forall i $$
                where
                \begin{itemize}[noitemsep]
                    \item $\omega$ : Normal vector to the hyperplane
                    \item $b$ : Bias term
                    \item $x_i, y_i$ : Training data points and labels
                \end{itemize}

                \nc{Kernel Trick}
                SVM can be extended to non-linearly separable data using kernel functions, which implicitly map input space to a higher-dimensional feature space.

                \nc{Common Kernels}
                \begin{itemize}[noitemsep]
                    \item Linear: $\langle x, x'\rangle$
                    \item Polynomial: $(\gamma \langle x, x'\rangle + r)^d$
                    \item Gaussian (RBF): $e^{(-\gamma \|x - x'\|^2)}$
                \end{itemize}

                (+) Effective in high-dimensional spaces, Memory efficient, Versatile (different kernel functions)\\
                
                (-) Sensitive to the choice of kernel and regularization parameters, Not suitable for very large datasets

                hinge loss: $max(0, 1-y_i(w \cdot x_i + b))$ (0 if correct classification)
                (1 if falls on the hyperplane)
                (>1 if misclassified)

               \nc{Objective function to min}
            $$ \min_{\omega, b} \quad \frac{1}{2} \|\omega\|^2 + C \sum_{i=1}^{n} \max(0, 1 - y_i(\omega \cdot x_i + b)) $$
            where $C$ nutch the hinge loss term (how far are we predicting from ground truth) and regularization term (impeach big value, min w => maximize margin)
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Support Vector Machine};
    \end{tikzpicture}
    %---------------------------------

%###############################################################################################
%                                         Deuxième Page
%###############################################################################################

    %--------------------------
    % Distance Measures
    %--------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
        \begin{minipage}{0.31\textwidth}
        $Pearson= R^{2} = 1 - \dfrac{\displaystyle\sum_{1}^{n}{\left(y_i - \hat{y_i}\right)^{2}}}{\displaystyle\sum_{1}^{n}{\left(y_i - \overline{y_i}\right)^{2}}}$\\
        $Euclidian = \sqrt{\displaystyle\sum{\left(I_1 - I_2\right)^{2}}}$\\
        $Manhattan = \displaystyle\sum{\abs{I_1 - I_2}}$\\
        $MSE = \dfrac{1}{N}\displaystyle\sum_{1}^{n}{\left(y_i - \hat{y_i}\right)^{2}}$\\
        $WSS = \displaystyle\sum{D^{2}\left(x_j,\mu_i\right)}$\\
        $cosine$ $similarity = \dfrac{A \cdot B}{\norm{A} \cdot \norm{B}}$\\
        \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Similarity Measures};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Clustering
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                clustering partition data into cluster with high intra-class similarity and low inter-class similarity.\\
                needs: (distance measure, criterion, algorithm)\\
                \nc{Partitions}
                \textbf{Distortion}: How close are we to a “centroid” defining the partition?\\
                \textbf{Connectivity of points}: How close are points to each other?
                \nc{Kmeans}
                \textbf{centroid} is the center of a cluster\\
                \textbf{codebook} is the ensemble of all centroids\\
                \textbf{partition} is the ensemble of samples attributed to a centroid (to a cluster)\\
                \nc{Mean Shift Clustering}
                \nc{DBSCAN}
                \nc{Hierarchical clustering}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Clustering};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % PCA
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                \nc{Kmeans}
                \nc{Hierarchical clustering}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Principal Component Analysis};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Autoencoders
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
                \nc{Kmeans}
                \nc{Hierarchical clustering}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Autoencoders};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Decision Tree
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
         \begin{minipage}{0.197\textwidth}
            \textbf{Concept}: 
            Decision tree is a flowchart-like structure in which each internal node represents a test on a feature (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all features).\\

            \textbf{Entropy}: 
            $$ H(X) = - \sum_{i=1}^{n} p(x_i) \log_2 p(x_i) $$
            where
            \begin{itemize}[noitemsep]
                \item $p(x_i)$ : Probability of class $x_i$
            \end{itemize}

            \textbf{Information Gain}: 
            $$ IG(X, Y) = H(X) - H(X|Y) $$
            where
            \begin{itemize}[noitemsep]
                \item $H(X)$ : Entropy of the parent node
                \item $H(X|Y)$ : Entropy of the child node
            \end{itemize}

            \textbf{Gini Impurity}: 
            $$ G(X) = 1 - \sum_{i=1}^{n} p(x_i)^2 $$
            where
            \begin{itemize}[noitemsep]
                \item $p(x_i)$ : Probability of class $x_i$
            \end{itemize}

            \textbf{CART Algorithm}: 
            \begin{itemize}[noitemsep]
                \item Select the best attribute using IG or Gini
                \item Make that attribute a decision node and break the dataset into smaller subsets
                \item Recursively repeat the process on each subset until you find leaf nodes in all the branches of the tree
            \end{itemize}

            \textbf{Pruning}: 
            Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances.\\

            (+) Easy to interpret and explain, Can handle both numerical and categorical data, Requires little data preparation, Able to handle multi-output problems, Uses a white box model, Can be used for feature selection, Performs well even if its assumptions are somewhat violated by the true model from which the data were generated\\
            
            (-) Overfitting, Can be unstable because small variations in the data might result in a completely different tree being generated, Not suitable for large datasets, Unbalanced datasets where one class is dominant, Biased trees if some classes dominate
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Decision Tree};
    \end{tikzpicture}

    %---------------------------------
    % Convolutional Neural Networks
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Convolutional Neural Networks};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Recurrent Neural Networks
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Recurrent Neural Networks};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Dimensionality Reduction
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Dimensionality Reduction};
    \end{tikzpicture}
    %---------------------------------

    %---------------------------------
    % Reinforcement Learning
    %---------------------------------
    \begin{tikzpicture}
        \node [mybox] (box){%
            \begin{minipage}{0.197\textwidth}
            \end{minipage}
        };
        \node[fancytitle, right=10pt] at (box.north west) {Reinforcement Learning};
    \end{tikzpicture}
    %---------------------------------










    
\end{multicols*}

%---------------------------------
% Computational Complexity of ML Algorithms
%---------------------------------
\begin{tikzpicture}
    \node [mybox] (box){%
        \begin{minipage}{0.95\textwidth}
            \begin{tabular}{|l|l|l|l|}
                \hline
                \textbf{Algorithm} & \textbf{Assumption} & \textbf{Train Time/Space} & \textbf{Inference Time/Space} \\
                \hline
                KNN (Brute Force) & Similar things exist in close proximity & \(O(knd)\) / \(O(nd)\) & \(O(knd)\) / \(O(nd)\) \\
                \hline
                KNN (KD Tree) & Similar things exist in close proximity & \(O(nd\log(n))\) / \(O(nd)\) & \(O(k\log(n)d)\) / \(O(nd)\) \\
                \hline
                Naive Bayes & Features are conditionally independent & \(O(ndc)\) / \(O(dc)\) & \(O(dc)\) / \(O(dc)\) \\
                \hline
                Logistic Regression & Classes are linearly separable & \(O(nd)\) / \(O(nd)\) & \(O(d)\) / \(O(d)\) \\
                \hline
                Linear Regression & Linear relationship between variables & \(O(nd)\) / \(O(nd)\) & \(O(d)\) / \(O(d)\) \\
                \hline
                SVM & Classes are linearly separable & \(O(n^2d^2)\) / \(O(nd)\) & \(O(kd)\) / \(O(kd)\) \\
                \hline
                Decision Tree & Feature selection by information gain & \(O(n\log(n)d)\) / \(O(\text{nodes})\) & \(O(\log(n))\) / \(O(\text{nodes})\) \\
                \hline
                Random Forest & Low bias and variance trees & \(O(kn\log(n)d)\) / \(O(\text{nodes} \times k)\) & \(O(k\log(n))\) / \(O(\text{nodes} \times k)\) \\
                \hline
                GBDT & High bias, low variance trees & \(O(Mn\log(n)d)\) / \(O(\text{nodes} \times M + \gamma_m)\) & \(O(M\log(n))\) / \(O(\text{nodes} \times M + \gamma_m)\) \\
                \hline
            \end{tabular}
        \end{minipage}
    };
    \node[fancytitle, right=10pt] at (box.north west) {Computational Complexity of ML Algorithms};
\end{tikzpicture}
%---------------------------------

    
\end{document}
